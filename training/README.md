# FluidDecoder Training System

åŸºäº ğŸ¤— Transformers çš„ FluidDecoder æ¨¡å‹è®­ç»ƒç³»ç»Ÿï¼Œä¸“ä¸ºå¤©ç„¶æ°”ç®¡ç½‘æµä½“åŠ¨åŠ›å­¦é¢„æµ‹è®¾è®¡ã€‚

## âœ¨ ç‰¹æ€§

- **å®Œæ•´çš„ HuggingFace é›†æˆ** - å…¼å®¹ `transformers.Trainer` å’Œç”Ÿæ€ç³»ç»Ÿ
- **é«˜çº§è®­ç»ƒæ§åˆ¶** - æ··åˆç²¾åº¦ã€åˆ†å¸ƒå¼è®­ç»ƒã€æ¢¯åº¦ç´¯ç§¯
- **ä¸“ä¸šæŒ‡æ ‡è¯„ä¼°** - è®¾å¤‡åˆ†ç»„æŒ‡æ ‡ã€æ—¶åºæŒ‡æ ‡ã€å¯è§†åŒ–ç›‘æ§
- **çµæ´»çš„æ¨ç†æ¥å£** - å•æ­¥é¢„æµ‹ã€æ‰¹é‡é¢„æµ‹ã€è‡ªå›å½’ç”Ÿæˆ
- **å®Œå–„çš„å®éªŒè·Ÿè¸ª** - TensorBoardã€WandB é›†æˆ

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºç¡€è®­ç»ƒ

```python
from transformers import Trainer, TrainingArguments
from training import (
    FluidDecoderForTraining, FluidDecoderConfig,
    FluidDataCollator, compute_fluid_metrics
)
from data import create_data_loaders

# 1. åˆ›å»ºæ¨¡å‹
config = FluidDecoderConfig(d_model=256, n_heads=8, n_layers=6)
model = FluidDecoderForTraining(config)

# 2. åŠ è½½æ•°æ®
train_loader, eval_loader = create_data_loaders("data/dataset")
train_dataset, eval_dataset = train_loader.dataset, eval_loader.dataset

# 3. è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=100,
    per_device_train_batch_size=32,
    learning_rate=1e-4,
    evaluation_strategy="steps",
    eval_steps=500,
    fp16=True,
)

# 4. åˆ›å»º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=FluidDataCollator(),
    compute_metrics=compute_fluid_metrics,
)

# 5. å¼€å§‹è®­ç»ƒ
trainer.train()
```

### å‘½ä»¤è¡Œè®­ç»ƒ

```bash
# åŸºç¡€è®­ç»ƒ
python training/scripts/train.py --model_size medium --epochs 50

# è‡ªå®šä¹‰å‚æ•°
python training/scripts/train.py \
    --d_model 512 --n_heads 16 --n_layers 12 \
    --batch_size 16 --learning_rate 5e-5 \
    --fp16 --gradient_checkpointing \
    --wandb --wandb_project "fluid-dynamics"

# ä»æ£€æŸ¥ç‚¹æ¢å¤
python training/scripts/train.py --resume_from_checkpoint "./results/checkpoint-1000"
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

```
training/
â”œâ”€â”€ hf_integration/          # HuggingFace é›†æˆ
â”‚   â”œâ”€â”€ model_wrapper.py     # FluidDecoderForTraining
â”‚   â”œâ”€â”€ data_collator.py     # FluidDataCollator  
â”‚   â”œâ”€â”€ metrics.py           # compute_fluid_metrics
â”‚   â””â”€â”€ callbacks.py         # è®­ç»ƒå›è°ƒå‡½æ•°
â”œâ”€â”€ scripts/                 # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ train.py            # ä¸»è®­ç»ƒè„šæœ¬
â”‚   â””â”€â”€ example_usage.py    # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ inference.py            # æ¨ç†æ¥å£
â””â”€â”€ README.md              # æœ¬æ–‡æ¡£
```

### æ•°æ®æµ

```
[B, T, V=6712] è¾“å…¥æ•°æ®
       â†“
FluidDataCollator æ‰¹å¤„ç†
       â†“  
FluidDecoderForTraining å‰å‘ä¼ æ’­
       â†“
[B, T, V=6712] é¢„æµ‹è¾“å‡º
       â†“
compute_fluid_metrics æŒ‡æ ‡è®¡ç®—
```

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### åŸºç¡€æŒ‡æ ‡
- **MSE**: å‡æ–¹è¯¯å·® (ä¸»è¦ä¼˜åŒ–ç›®æ ‡)
- **MAE**: å¹³å‡ç»å¯¹è¯¯å·®
- **RMSE**: å‡æ–¹æ ¹è¯¯å·®
- **RÂ²**: å†³å®šç³»æ•°
- **MAPE**: å¹³å‡ç»å¯¹ç™¾åˆ†æ¯”è¯¯å·®

### è®¾å¤‡åˆ†ç»„æŒ‡æ ‡
```python
equipment_metrics = {
    'b_mse': 0.0023,    # çƒé˜€ MSE
    'c_mse': 0.0015,    # å‹ç¼©æœº MSE
    'h_mse': 0.0031,    # ç®¡æ®µ MSE
    'n_mse': 0.0019,    # èŠ‚ç‚¹ MSE
    'p_mse': 0.0027,    # ç®¡é“ MSE
    'r_mse': 0.0012,    # è°ƒèŠ‚é˜€ MSE
    # ... å¯¹åº”çš„ MAE, RMSE æŒ‡æ ‡
}
```

## âš™ï¸ é…ç½®é€‰é¡¹

### æ¨¡å‹é…ç½®

```python
config = FluidDecoderConfig(
    d_model=256,                    # éšè—å±‚ç»´åº¦
    n_heads=8,                     # æ³¨æ„åŠ›å¤´æ•°
    n_layers=6,                    # Decoderå±‚æ•°
    d_ff=1024,                     # å‰é¦ˆç½‘ç»œç»´åº¦
    input_dim=6712,                # è¾“å…¥ç»´åº¦ (å›ºå®š)
    output_dim=6712,               # è¾“å‡ºç»´åº¦ (å›ºå®š)
    boundary_dims=538,             # è¾¹ç•Œæ¡ä»¶ç»´åº¦
    dropout=0.1,                   # Dropoutæ¦‚ç‡
    activation="gelu",             # æ¿€æ´»å‡½æ•°
    max_time_positions=10,         # æœ€å¤§æ—¶é—´ä½ç½®
    max_variable_positions=6712,   # æœ€å¤§å˜é‡ä½ç½®
)
```

### é¢„å®šä¹‰æ¨¡å‹å¤§å°

| å¤§å° | d_model | n_heads | n_layers | d_ff | å‚æ•°é‡ |
|------|---------|---------|----------|------|--------|
| small | 128 | 4 | 3 | 512 | ~2M |
| medium | 256 | 8 | 6 | 1024 | ~8M |
| large | 512 | 16 | 12 | 2048 | ~32M |

### è®­ç»ƒå‚æ•°

```python
training_args = TrainingArguments(
    # è®­ç»ƒæ§åˆ¶
    num_train_epochs=100,
    per_device_train_batch_size=32,
    gradient_accumulation_steps=1,
    
    # ä¼˜åŒ–å™¨
    optim="adamw_torch",
    learning_rate=1e-4,
    weight_decay=1e-5,
    lr_scheduler_type="cosine",
    warmup_steps=500,
    
    # æ€§èƒ½ä¼˜åŒ–
    fp16=True,                     # æ··åˆç²¾åº¦
    gradient_checkpointing=True,   # å†…å­˜ä¼˜åŒ–
    dataloader_num_workers=4,
    
    # è¯„ä¼°å’Œä¿å­˜
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=1000,
    load_best_model_at_end=True,
    
    # å®éªŒè·Ÿè¸ª
    report_to=["wandb", "tensorboard"],
)
```

## ğŸ“ˆ ç›‘æ§å’Œå¯è§†åŒ–

### è®­ç»ƒå›è°ƒ

```python
from training.hf_integration.callbacks import create_training_callbacks

callbacks = create_training_callbacks(
    save_plots=True,        # ä¿å­˜æŒ‡æ ‡å›¾è¡¨
    monitor_memory=False,   # ç›‘æ§å†…å­˜ä½¿ç”¨
)

trainer = Trainer(..., callbacks=callbacks)
```

### ç”Ÿæˆçš„å›¾è¡¨

è®­ç»ƒè¿‡ç¨‹ä¼šè‡ªåŠ¨ç”Ÿæˆä»¥ä¸‹å¯è§†åŒ–ï¼š

1. **è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿**
2. **MSE å’Œ MAE è¶‹åŠ¿**
3. **RÂ² å†³å®šç³»æ•°å˜åŒ–**
4. **è®¾å¤‡åˆ†ç»„æŒ‡æ ‡å¯¹æ¯”**

å›¾è¡¨ä¿å­˜åœ¨ `{output_dir}/training_metrics_*.png`

### å®éªŒè·Ÿè¸ª

```python
# WandB é›†æˆ
trainer = Trainer(..., args=TrainingArguments(
    report_to=["wandb"],
    run_name="experiment_v1",
))

# TensorBoard é›†æˆ (è‡ªåŠ¨å¯ç”¨)
# æŸ¥çœ‹: tensorboard --logdir ./logs
```

## ğŸ”® æ¨ç†æ¥å£

### åŸºç¡€æ¨ç†

```python
from training import FluidInference

# åŠ è½½æ¨¡å‹
inference = FluidInference(
    model_path="./results/final_model",
    normalizer_path="./data/normalizer.pkl",  # å¯é€‰
    device="cuda"
)

# å•æ ·æœ¬é¢„æµ‹
result = inference.predict_single(
    input_data,        # [T, V=6712]
    denormalize=True   # åå½’ä¸€åŒ–ç»“æœ
)
print(f"é¢„æµ‹ç»“æœå½¢çŠ¶: {result['predictions'].shape}")
```

### æ‰¹é‡æ¨ç†

```python
# æ‰¹é‡é¢„æµ‹
results = inference.predict_batch(
    input_batch,       # [B, T, V=6712]  
    batch_size=32,
    denormalize=True
)
```

### è‡ªå›å½’ç”Ÿæˆ

```python
# å¤šæ­¥é¢„æµ‹
results = inference.predict_autoregressive(
    initial_input,     # [T, V=6712]
    steps=10,          # é¢„æµ‹10æ­¥
    denormalize=True
)
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### åˆ†å¸ƒå¼è®­ç»ƒ

```bash
# å¤šGPUè®­ç»ƒ
python -m torch.distributed.launch --nproc_per_node=4 \
    training/scripts/train.py --batch_size 8

# å¤šæœºè®­ç»ƒ
python -m torch.distributed.launch \
    --nproc_per_node=4 --nnodes=2 --node_rank=0 \
    --master_addr="192.168.1.1" --master_port=12355 \
    training/scripts/train.py
```

### è¶…å‚æ•°æœç´¢

```python
from transformers import Trainer
import optuna

def objective(trial):
    # å®šä¹‰è¶…å‚æ•°æœç´¢ç©ºé—´
    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-3)
    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])
    d_model = trial.suggest_categorical('d_model', [128, 256, 512])
    
    # åˆ›å»ºé…ç½®å’Œè®­ç»ƒ
    config = FluidDecoderConfig(d_model=d_model)
    model = FluidDecoderForTraining(config)
    
    training_args = TrainingArguments(
        learning_rate=lr,
        per_device_train_batch_size=batch_size,
        # ... å…¶ä»–å‚æ•°
    )
    
    trainer = Trainer(model=model, args=training_args, ...)
    trainer.train()
    
    # è¿”å›ä¼˜åŒ–ç›®æ ‡
    eval_result = trainer.evaluate()
    return eval_result['eval_mse']

# è¿è¡Œä¼˜åŒ–
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)
```

### è‡ªå®šä¹‰æŒ‡æ ‡

```python
from training.hf_integration.metrics import create_metrics_computer

# åˆ›å»ºè‡ªå®šä¹‰æŒ‡æ ‡è®¡ç®—å™¨
compute_metrics = create_metrics_computer(
    include_equipment=True,   # åŒ…å«è®¾å¤‡æŒ‡æ ‡
    include_temporal=False,   # ä¸åŒ…å«æ—¶åºæŒ‡æ ‡
)

trainer = Trainer(..., compute_metrics=compute_metrics)
```

## ğŸ› è°ƒè¯•å’Œæ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **å†…å­˜ä¸è¶³**
   ```python
   training_args = TrainingArguments(
       gradient_checkpointing=True,
       per_device_train_batch_size=8,  # å‡å°æ‰¹æ¬¡å¤§å°
       gradient_accumulation_steps=4,  # å¢åŠ æ¢¯åº¦ç´¯ç§¯
   )
   ```

2. **è®­ç»ƒä¸æ”¶æ•›**
   ```python
   training_args = TrainingArguments(
       learning_rate=5e-5,      # é™ä½å­¦ä¹ ç‡
       warmup_steps=1000,       # å¢åŠ é¢„çƒ­æ­¥æ•°
       lr_scheduler_type="linear",  # ä½¿ç”¨çº¿æ€§è¡°å‡
   )
   ```

3. **è¯„ä¼°æŒ‡æ ‡å¼‚å¸¸**
   - æ£€æŸ¥æ•°æ®é¢„å¤„ç†å’Œå½’ä¸€åŒ–
   - éªŒè¯é¢„æµ‹æ©ç è®¾ç½®
   - æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„è­¦å‘Šä¿¡æ¯

### è°ƒè¯•æ¨¡å¼

```bash
# å¯ç”¨è°ƒè¯•æ¨¡å¼
python training/scripts/train.py --debug --epochs 1

# ä½¿ç”¨å°æ•°æ®é›†æµ‹è¯•
python training/scripts/example_usage.py
```

## ğŸ“ æœ€ä½³å®è·µ

### è®­ç»ƒç­–ç•¥

1. **æ¸è¿›å¼è®­ç»ƒ**: å…ˆç”¨å°æ¨¡å‹å’Œå°‘é‡æ•°æ®éªŒè¯æµç¨‹
2. **å­¦ä¹ ç‡è°ƒä¼˜**: ä½¿ç”¨å­¦ä¹ ç‡æŸ¥æ‰¾å™¨ç¡®å®šæœ€ä½³å­¦ä¹ ç‡
3. **æ‰¹æ¬¡å¤§å°**: æ ¹æ®GPUå†…å­˜é€‰æ‹©åˆé€‚çš„æ‰¹æ¬¡å¤§å°
4. **æ—©åœç­–ç•¥**: è®¾ç½®åˆç†çš„æ—©åœè€å¿ƒå€¼é¿å…è¿‡æ‹Ÿåˆ

### æ¨¡å‹é…ç½®

1. **æ¨¡å‹å¤§å°é€‰æ‹©**: 
   - åŸå‹é˜¶æ®µ: `small`
   - å®éªŒé˜¶æ®µ: `medium`  
   - ç”Ÿäº§é˜¶æ®µ: `large`

2. **è¶…å‚æ•°è®¾ç½®**:
   - `d_model`: é€šå¸¸è®¾ä¸º64çš„å€æ•°
   - `n_heads`: åº”èƒ½è¢«`d_model`æ•´é™¤
   - `d_ff`: é€šå¸¸ä¸º`d_model`çš„4å€

### æ•°æ®å¤„ç†

1. **æ•°æ®å½’ä¸€åŒ–**: ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„å½’ä¸€åŒ–å‚æ•°
2. **æ©ç è®¾ç½®**: è¾¹ç•Œæ¡ä»¶ä¸åº”å‚ä¸æŸå¤±è®¡ç®—
3. **æ—¶åºå¯¹é½**: ç¡®ä¿è¾“å…¥å’Œç›®æ ‡çš„æ—¶åºå¯¹åº”å…³ç³»

## ğŸ“š å‚è€ƒèµ„æ–™

- [HuggingFace Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers/)
- [PyTorch åˆ†å¸ƒå¼è®­ç»ƒæŒ‡å—](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [Weights & Biases é›†æˆ](https://docs.wandb.ai/guides/integrations/huggingface)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Request æ¥æ”¹è¿›è®­ç»ƒç³»ç»Ÿï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®åŸºäº MIT è®¸å¯è¯å¼€æºã€‚